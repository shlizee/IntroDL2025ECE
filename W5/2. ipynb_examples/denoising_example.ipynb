{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "25eb49c4",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> LAB 5: </span>\n",
    "# <span style='color:blue'> ADVANCED RECURRENT NEURAL NETWORKS </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8966ac77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math, random\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4519be02",
   "metadata": {},
   "source": [
    "## <span style='color:red'> Signal Denoising </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36b79c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seaborn plot styling\n",
    "sns.set(style = 'white', font_scale = 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2369ec24",
   "metadata": {},
   "source": [
    "## Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91536af0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sinusoidal wave generator\n",
    "def sinusoidal_generator(X, signal_freq=60.):\n",
    "    \n",
    "    return np.sin(2 * np.pi * (X) / signal_freq)\n",
    "\n",
    "# Adds uniform noise to the provided signal with particular noise range\n",
    "def add_noise(Y, noise_range=(-0.35, 0.35)):\n",
    "    \n",
    "    noise = np.random.uniform(noise_range[0], noise_range[1], size=Y.shape)\n",
    "    \n",
    "    return Y + noise\n",
    "\n",
    "# Randomly generates a noised and denoised sinusoidal sequence pair given the sequence length\n",
    "# The function uses the two functions above\n",
    "def sample_seq(sequence_length):\n",
    "    \n",
    "    random_offset = random.randint(0, sequence_length)\n",
    "    X = np.arange(sequence_length)\n",
    "    \n",
    "    denoised_output_seq = sinusoidal_generator(X + random_offset)\n",
    "    noisy_input_seq = add_noise(denoised_output_seq)\n",
    "    \n",
    "    return noisy_input_seq, denoised_output_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25e16c8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate noised/denoised signal pair using the sample_seq function using sequence length = 100\n",
    "noisy_input_seq, denoised_output_seq = sample_seq(sequence_length = 100)\n",
    "\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.plot(noisy_input_seq, label ='Noisy', linewidth = 3)\n",
    "plt.plot(denoised_output_seq, label ='Denoised', linewidth = 3)\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d3dd6ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uses the sample_seq function above to generate n-samples of noisy and denoised sequence pairs\n",
    "def create_synthetic_dataset(n_samples, sequence_length):\n",
    "    \n",
    "    noisy_seq_inputs = np.zeros((n_samples, sequence_length))\n",
    "    denoised_seq_outputs = np.zeros((n_samples, sequence_length))\n",
    "    \n",
    "    for i in range(n_samples):\n",
    "        \n",
    "        noisy_inp, denoised_out = sample_seq(sequence_length)\n",
    "        \n",
    "        noisy_seq_inputs[i, :] = noisy_inp\n",
    "        denoised_seq_outputs[i, :] = denoised_out\n",
    "        \n",
    "    return noisy_seq_inputs, denoised_seq_outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74d203ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create 12000 pairs of noisy/denoised sequences with sequence length of 100\n",
    "noisy_seq_inputs, denoised_seq_outputs = create_synthetic_dataset(n_samples = 12000, sequence_length = 100)\n",
    "\n",
    "# Take first 8000 pairs as training (noisy: inputs, denoised: targets)\n",
    "train_input_seqs, train_output_seqs = noisy_seq_inputs[:8000], denoised_seq_outputs[:8000]\n",
    "# Take last 4000 pairs as testing (noisy: inputs, denoised: targets)\n",
    "test_input_seqs, test_output_seqs = noisy_seq_inputs[8000:], denoised_seq_outputs[8000:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62e3153",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reshape training and testing to conform to (sample_size, sequence_length, # of features) format by PyTorch\n",
    "train_input_seqs = train_input_seqs.reshape((train_input_seqs.shape[0], -1, 1))\n",
    "train_output_seqs = train_output_seqs.reshape((train_output_seqs.shape[0], -1, 1))\n",
    "\n",
    "test_input_seqs = test_input_seqs.reshape((test_input_seqs.shape[0], -1, 1))\n",
    "test_output_seqs = test_output_seqs.reshape((test_output_seqs.shape[0], -1, 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074d2a3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Training Input Data Shape: \", train_input_seqs.shape)\n",
    "print(\"Training Output Data Shape: \", train_output_seqs.shape)\n",
    "print(\"Testing Input Data Shape: \", test_input_seqs.shape)\n",
    "print(\"Testing Output Data Shape: \", test_output_seqs.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c955a1fc",
   "metadata": {},
   "source": [
    "## Define Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b4e68",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Denoiser_GRU(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, input_size, hidden_size, num_layers, output_size):\n",
    "        \n",
    "        super(Denoiser_GRU, self).__init__()\n",
    "        \n",
    "        # Using GRU cell with batch_first = True, i.e., we are using (sample_size, sequence_length, # of features) format\n",
    "        # Bidirectional = False, so only forward context is taken\n",
    "        self.gru = torch.nn.GRU(input_size=input_size, hidden_size=hidden_size, \n",
    "                                num_layers = num_layers, \n",
    "                                batch_first = True,\n",
    "                                bidirectional = False)\n",
    "        \n",
    "        # Decoder layer to take in GRU hidden states and output a single number (i.e. denoised value at time t)\n",
    "        self.decoder = torch.nn.Linear(hidden_size, output_size) \n",
    "        \n",
    "        # Uses Tanh activation to squeeze the decoder output between -1 and 1\n",
    "        # Note that our dataset consists of sinusoidal between -1 and 1\n",
    "        self.output_activation = torch.nn.Tanh()\n",
    "        \n",
    "    def forward(self, input_seq, hidden_state):\n",
    "        \n",
    "        # Feed gru cell with input sequence and initial hidden state\n",
    "        pred, hidden = self.gru(input_seq, hidden_state)\n",
    "        \n",
    "        # Feed the output of the GRU (pred) to decoder layer, followed by taking the Tanh activation\n",
    "        pred = self.output_activation(self.decoder(pred))\n",
    "        \n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05c0273f",
   "metadata": {},
   "source": [
    "## Define Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b767b333",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define our RNN sequence model. Using hidden size = 30 and single layer of GRU layer (num_layers = 1) \n",
    "\n",
    "denoiser_GRU = Denoiser_GRU(input_size = 1, hidden_size = 30, \n",
    "                            num_layers = 1, output_size = 1)\n",
    "\n",
    "# Define learning rate and epochs\n",
    "learning_rate = 0.0003          \n",
    "epochs = 100\n",
    "\n",
    "# Batch size for mini-batch gradient\n",
    "batchsize = 300\n",
    "\n",
    "# Using least absolute deviation loss L1Loss\n",
    "# The loss function minimizes the sum of the all the absolute differences between the true value and the predicted value\n",
    "# Using Adam as optimizer\n",
    "loss_func = torch.nn.L1Loss()\n",
    "optimizer = torch.optim.Adam(denoiser_GRU.parameters(), lr=learning_rate)\n",
    "\n",
    "# Attach .cuda() if running with GPU\n",
    "# Make sure the displayed info matches what you specified above\n",
    "denoiser_GRU"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f29d00",
   "metadata": {},
   "source": [
    "## Identify Tracked Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89392be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Empty Python list to append training loss values\n",
    "train_loss_list = []"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ed1806f",
   "metadata": {},
   "source": [
    "## Train Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a64ee3a1",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Convert training/testing data to torch tensors\n",
    "# Using .float() for both since our targets are also floating point numbers\n",
    "# Attach .cuda() to use with GPU\n",
    "train_input_seqs = torch.from_numpy(train_input_seqs).float()\n",
    "train_output_seqs = torch.from_numpy(train_output_seqs).float()\n",
    "\n",
    "test_input_seqs = torch.from_numpy(test_input_seqs).float()\n",
    "test_output_seqs = torch.from_numpy(test_output_seqs).float()\n",
    "\n",
    "# Split the training features/targets to mini-batch size defined above\n",
    "train_batches_features = torch.split(train_input_seqs, batchsize)\n",
    "train_batches_targets = torch.split(train_output_seqs, batchsize)\n",
    "\n",
    "# Total number of mini-batches in the training set\n",
    "batch_split_num = len(train_batches_features)\n",
    "\n",
    "for epoch in range(epochs): # For each epoch\n",
    "    \n",
    "    for k in range(batch_split_num): # iterate through mini-batches\n",
    "        \n",
    "        # initialize the hidden states as None\n",
    "        hidden_state = None\n",
    "        \n",
    "        # Feed in k-th mini-batch training features to GRU + hidden states to produce output\n",
    "        pred = denoiser_GRU(train_batches_features[k], hidden_state)\n",
    "        \n",
    "        # zero_grad() to clear gradient buffer\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # Compute loss between the GRU denoised sequence vs target training sequence\n",
    "        loss = loss_func(pred, train_batches_targets[k])\n",
    "        # Append loss\n",
    "        train_loss_list.append(loss.item())\n",
    "        \n",
    "        # Backpropagation\n",
    "        loss.backward()\n",
    "        \n",
    "        # Update weights/biases\n",
    "        optimizer.step()\n",
    "    \n",
    "    # Print the averaged training loss (L1 Loss) throughout epoch\n",
    "    print(\"Averaged Training Loss for Epoch \", epoch,\": \", np.mean(train_loss_list[-batch_split_num:]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0c5dacd",
   "metadata": {},
   "source": [
    "## Visualize & Evaluate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d036db29",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize training loss curve\n",
    "plt.figure(figsize = (10, 5))\n",
    "\n",
    "plt.plot(train_loss_list, linewidth = 3, label = 'Training Loss')\n",
    "plt.ylabel(\"training loss\")\n",
    "plt.xlabel(\"Iterations\")\n",
    "plt.legend()\n",
    "sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813be719",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the L1 Loss for testing set by feeding in the test_input_seqs defined earlier to the trained GRU\n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    # test_prediction: Denoised test sequences processed by trained GRU\n",
    "    test_prediction = denoiser_GRU(test_input_seqs, None)\n",
    "    test_loss = loss_func(test_prediction, test_output_seqs).item()\n",
    "    \n",
    "    print(\"Testing Loss (Least Absolute Deviations): \", test_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aada12db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's randomly sample the test prediction generated by trained GRU vs the expected the target\n",
    "# If using GPU need to attach .cpu() first before you use convert back to numpy arrays\n",
    "\n",
    "sample_num = 1000 # Feel free to vary this number between 0 and 4000 \n",
    "\n",
    "with torch.no_grad():\n",
    "    \n",
    "    plt.figure(figsize = (10, 5))\n",
    "    \n",
    "    plt.plot(test_prediction[sample_num].numpy(), label='RNN Denoised', linewidth = 3)\n",
    "    plt.plot(test_output_seqs[sample_num].numpy(), label='Groundtruth', linewidth = 3)\n",
    "    plt.legend()\n",
    "    plt.title(\"Sample num: {}\".format(sample_num))\n",
    "    sns.despine()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3be7d361",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
