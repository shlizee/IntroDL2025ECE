{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8ccac5f2-278e-4fb7-9898-b65ea4cc419a",
   "metadata": {},
   "source": [
    "# <span style='color:blue'> LAB 9: </span>\n",
    "# <span style='color:blue'> DEEP REINFORCEMENT LEARNING </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ea16a68-b49b-44ff-a3d0-c6e574b83ca7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "import numpy as np\n",
    "import random \n",
    "from collections import namedtuple, deque "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "715c7ac9-3b85-463c-9b22-0d284b0d0392",
   "metadata": {},
   "source": [
    "## Policy/target network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f3f6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Q_network(nn.Module):\n",
    "\n",
    "    def __init__(self, state_size,action_size, seed, fc1_unit,\n",
    "                 fc2_unit):\n",
    "\n",
    "        super(QNetwork,self).__init__() \n",
    "        self.seed = torch.manmual_seed(seed)\n",
    "        self.fc1= nn.Linear(state_size,fc1_unit)\n",
    "        seed.fc2 = nn.Linear(fc1_unit,fc2_unit)\n",
    "        seed.fc3 = nn.Linear(fc2_unit,action_size)\n",
    "        \n",
    "    def forward(self,x):\n",
    "\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = F.relu(self.fc2(x))\n",
    "        return self.fc3(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1d2214a1-d61d-4733-b8f6-e1eeffdd2dea",
   "metadata": {},
   "source": [
    "## Define Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7fe2b3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent():\n",
    "    \n",
    "    def __init__(self, state_size, action_size, seed):\n",
    "        \n",
    "        self.state_size = state_size\n",
    "        self.action_size = action_size\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "        #Q- Network\n",
    "        self.qnetwork_policy = Q_network(state_size, action_size, seed).to(device)\n",
    "        self.qnetwork_target = Q_network(state_size, action_size, seed).to(device)\n",
    "        \n",
    "        self.optimizer = optim.Adam(self.qnetwork_policy.parameters(),lr=LR)\n",
    "        \n",
    "        # Replay memory \n",
    "        self.memory = ExperienceRelay(action_size, BUFFER_SIZE,BATCH_SIZE,seed)\n",
    "        # Initialize time step (for updating every UPDATE_EVERY steps)\n",
    "        self.t_step = 0\n",
    "        \n",
    "    def step(self, state, action, reward, next_step, done):\n",
    "        # Save experience in replay memory\n",
    "        self.memory.add(state, action, reward, next_step, done)\n",
    "\n",
    "        # Learn every UPDATE_EVERY time steps.\n",
    "        self.t_step = (self.t_step+1)% UPDATE_EVERY\n",
    "        if self.t_step == 0:\n",
    "            # If enough samples are available in memory, get radom subset and learn\n",
    "\n",
    "            if len(self.memory)>BATCH_SIZE:\n",
    "                experience = self.memory.sample()\n",
    "                self.learn(experience, GAMMA)\n",
    "                \n",
    "    def act(self, state, eps = 0):\n",
    "\n",
    "        state = torch.from_numpy(state).float().unsqueeze(0).to(device)\n",
    "        self.qnetwork_policy.eval()\n",
    "        with torch.no_grad():\n",
    "            action_values = self.qnetwork_policy(state)\n",
    "        self.qnetwork_policy.train()\n",
    "\n",
    "        #Epsilon -greedy action selction\n",
    "        if random.random() > eps:\n",
    "            return np.argmax(action_values.cpu().data.numpy())\n",
    "        else:\n",
    "            return random.choice(np.arange(self.action_size))\n",
    "            \n",
    "    def learn(self, experiences, gamma):\n",
    "\n",
    "        states, actions, rewards, next_state, dones = experiences\n",
    "        criterion = torch.nn.MSELoss()\n",
    "        self.qnetwork_policy.train()\n",
    "        self.qnetwork_target.eval()\n",
    "        predicted_targets = self.qnetwork_policy(states).gather(1,actions)\n",
    "    \n",
    "        with torch.no_grad():\n",
    "            labels_next = self.qnetwork_target(next_states).detach().max(1)[0].unsqueeze(1)\n",
    "\n",
    "        labels = rewards + (gamma* labels_next*(1-dones))\n",
    "        \n",
    "        loss = criterion(predicted_targets,labels).to(device)\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "        # ------------------- update target network ------------------- #\n",
    "        self.soft_update(self.qnetwork_policy,self.qnetwork_target,TAU)\n",
    "            \n",
    "    def soft_update(self, local_model, target_model, tau):\n",
    "\n",
    "        for target_param, local_param in zip(target_model.parameters(),\n",
    "                                           local_model.parameters()):\n",
    "            target_param.data.copy_(tau*local_param.data + (1-tau)*target_param.data)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "636c23e5-0c15-4394-8c35-88c3ed834dfa",
   "metadata": {},
   "source": [
    "## Define Experience relay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0512808e-2a2d-4e22-bd27-9adaa6533e3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceRelay:\n",
    "    \n",
    "    def __init__(self, action_size, buffer_size, batch_size, seed):\n",
    "        \n",
    "        self.action_size = action_size\n",
    "        self.memory = deque(maxlen=buffer_size)\n",
    "        self.batch_size = batch_size\n",
    "        self.experiences = namedtuple(\"Experience\", field_names=[\"state\",\n",
    "                                                               \"action\",\n",
    "                                                               \"reward\",\n",
    "                                                               \"next_state\",\n",
    "                                                               \"done\"])\n",
    "        self.seed = random.seed(seed)\n",
    "        \n",
    "    def add(self,state, action, reward, next_state,done):\n",
    "        e = self.experiences(state,action,reward,next_state,done)\n",
    "        self.memory.append(e)\n",
    "        \n",
    "    def sample(self):\n",
    "        experiences = random.sample(self.memory,k=self.batch_size)\n",
    "        \n",
    "        states = torch.from_numpy(np.vstack([e.state for e in experiences if e is not None])).float().to(device)\n",
    "        actions = torch.from_numpy(np.vstack([e.action for e in experiences if e is not None])).long().to(device)\n",
    "        rewards = torch.from_numpy(np.vstack([e.reward for e in experiences if e is not None])).float().to(device)\n",
    "        next_states = torch.from_numpy(np.vstack([e.next_state for e in experiences if e is not None])).float().to(device)\n",
    "        dones = torch.from_numpy(np.vstack([e.done for e in experiences if e is not None]).astype(np.uint8)).float().to(device)\n",
    "        \n",
    "        return (states,actions,rewards,next_states,dones)\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.memory)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8ec2734-4a8d-4a8b-aef6-7766e9816d67",
   "metadata": {},
   "source": [
    "## Example hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8665fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "BUFFER_SIZE = int(1e5)  # replay buffer size\n",
    "BATCH_SIZE = 64         # minibatch size\n",
    "GAMMA = 0.99            # reward discount factor\n",
    "TAU = 1e-3              # for soft update of target parameters (0 -> hard update where target net == policy net)\n",
    "LR = 5e-4               # learning rate\n",
    "UPDATE_EVERY = 4        # how often to update the network\n",
    "\n",
    "# Also see original publication https://www.nature.com/articles/nature14236"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
